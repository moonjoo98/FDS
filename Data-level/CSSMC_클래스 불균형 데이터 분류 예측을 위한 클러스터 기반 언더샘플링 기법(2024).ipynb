{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cO_bqbeqHDHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "id": "NCxMXKmUHJnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhwtFPO1Gsea"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "file_name = \"./data/creditcard.csv.zip\"\n",
        "output_dir = \"./data\"\n",
        "format = \"zip\"\n",
        "shutil.unpack_archive(file_name, output_dir, format)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.model_selection  import cross_val_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "dNzgcTZKHdGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./data/creditcard.csv',sep=',')"
      ],
      "metadata": {
        "id": "v7dm6Ns_Hk8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['Class']\n",
        "X = df.drop([\"Class\",'Time'], axis=1)\n"
      ],
      "metadata": {
        "id": "R5CiTmtVIMl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.3)\n"
      ],
      "metadata": {
        "id": "ETw21JbWMElC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_train.columns, index=X_test.index)"
      ],
      "metadata": {
        "id": "jGtDwGAyT8u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YV4k11pwcyXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_majority = X_train_scaled_df[y_train == 0]\n",
        "y_majority = y_train[y_train == 0]\n",
        "\n",
        "# 2. k-means 클러스터링\n",
        "n_clusters = 15  # 클러스터 개수를 15로 설정합니다.\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X_majority)\n",
        "clusters = kmeans.predict(X_majority)\n",
        "\n",
        "\n",
        "cluster_data_list = [X_majority[clusters == i] for i in range(n_clusters)]\n",
        "cluster_sizes = [len(cluster_data) for cluster_data in cluster_data_list]\n",
        "\n",
        "# 3. 여러 개의 랜덤 서브셋 생성\n",
        "num_subsets = 10  # 생성할 서브셋의 개수\n",
        "total_sample_size = 20000  # 총 샘플 수\n",
        "all_sampled_subsets = []\n",
        "\n",
        "def sample_without_replacement(cluster_data, n_samples, used_indices):\n",
        "    available_indices = list(set(range(len(cluster_data))) - used_indices)\n",
        "    if len(available_indices) < n_samples:\n",
        "        n_samples = len(available_indices)\n",
        "    sampled_indices = np.random.choice(available_indices, size=n_samples, replace=False)\n",
        "    used_indices.update(sampled_indices)\n",
        "    return cluster_data.iloc[sampled_indices]\n",
        "\n",
        "for _ in range(num_subsets):\n",
        "    remaining_samples = total_sample_size\n",
        "    final_sampled_subset = pd.DataFrame()\n",
        "    used_indices_list = [set() for _ in range(n_clusters)]\n",
        "\n",
        "    while remaining_samples > 0:\n",
        "        for i, cluster_data in enumerate(cluster_data_list):\n",
        "            if remaining_samples <= 0:\n",
        "                break\n",
        "            available_samples = len(cluster_data) - len(used_indices_list[i])\n",
        "            if available_samples > 0:\n",
        "                if cluster_sizes[i] < 1300:\n",
        "                    n_samples = available_samples\n",
        "                else:\n",
        "                    n_samples = min(1300, available_samples, remaining_samples)\n",
        "                sampled_data = sample_without_replacement(cluster_data, n_samples, used_indices_list[i])\n",
        "                final_sampled_subset = pd.concat([final_sampled_subset, sampled_data])\n",
        "                remaining_samples -= n_samples\n",
        "            if len(final_sampled_subset) >= total_sample_size:\n",
        "                final_sampled_subset = final_sampled_subset.iloc[:total_sample_size]\n",
        "                remaining_samples = 0\n",
        "                break\n",
        "    all_sampled_subsets.append(final_sampled_subset)\n"
      ],
      "metadata": {
        "id": "koQM2WbBsUk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import slogdet, inv\n",
        "import numpy as np\n",
        "\n",
        "def kl_divergence(mu_p, sigma_p, mu_q, sigma_q):\n",
        "    \"\"\"\n",
        "    Calculate the KL divergence between two multivariate Gaussian distributions.\n",
        "\n",
        "    Parameters:\n",
        "    mu_p (np.ndarray): Mean vector of the distribution p\n",
        "    sigma_p (np.ndarray): Covariance matrix of the distribution p\n",
        "    mu_q (np.ndarray): Mean vector of the distribution q\n",
        "    sigma_q (np.ndarray): Covariance matrix of the distribution q\n",
        "\n",
        "    Returns:\n",
        "    float: The KL divergence D_KL(p || q)\n",
        "    \"\"\"\n",
        "    D = mu_p.shape[0]  # Dimension of the distributions\n",
        "\n",
        "    # Calculate the determinant and inverse of sigma_q\n",
        "    det_sigma_p = np.linalg.det(sigma_p)\n",
        "    det_sigma_q = np.linalg.det(sigma_q)\n",
        "    inv_sigma_q = np.linalg.inv(sigma_q)\n",
        "\n",
        "    # Calculate the first term: 0.5 * log(det_sigma_q / det_sigma_p)\n",
        "    term1 = 0.5 * np.log(det_sigma_q / det_sigma_p) # 두 분포의 공분산 행렬의 행렬식 비율의 로그. 공분산 행렬의 행렬식은 분포의 \"확산 정도\"를 나타내므로, 두 분포의 확산 정도 차이를 로그 비율\n",
        "\n",
        "    # Calculate the second term: -D/2\n",
        "    term2 = -0.5 * D # 분포의 차원 D에 대한 조정 항, KL 발산 값이 분포의 차원에 대해 조정\n",
        "\n",
        "    # Calculate the third term: 0.5 * trace(inv_sigma_q @ sigma_p)\n",
        "    term3 = 0.5 * np.trace(inv_sigma_q @ sigma_p) # 분포 q의 공분산 행렬의 역행렬과 분포 p의 공분산 행렬의 곱의 대각합 공분산 행렬의 곱의 대각합은 두 분포의 상관 관계와 분산의 차이\n",
        "\n",
        "    # Calculate the fourth term: 0.5 * (mu_p - mu_q).T @ inv_sigma_q @ (mu_p - mu_q)\n",
        "    diff_mu = mu_p - mu_q\n",
        "    term4 = 0.5 * diff_mu.T @ inv_sigma_q @ diff_mu # 두 분포의 평균 벡터 차이에 대한 조정 항, @는 행렬 곱셈 연산자, 이 항은 두 분포의 중심(평균 벡터)의 차이를 측정합니다. 두 분포의 평균 벡터가 얼마나 다른지.\n",
        "\n",
        "    # Sum all the terms to get the KL divergence\n",
        "    kl_div = term1 + term2 + term3 + term4\n",
        "\n",
        "    return kl_div\n",
        "\n",
        "kl_divergences = []\n",
        "mu_p = X_majority.mean().values\n",
        "sigma_p = np.cov(X_majority, rowvar=False) # 원본 데이터의 공분산 행렬\n",
        "\n",
        "for subset in all_sampled_subsets:\n",
        "    mu_q = subset.mean().values\n",
        "    sigma_q = np.cov(subset, rowvar=False) # Subest의 공분산 행렬\n",
        "    kl_divergences.append(kl_divergence(mu_p, sigma_p, mu_q, sigma_q)) # 원본 데이터와 subset의 mu(모집단 평균)와 sigma(공분산 행렬) 비교\n",
        "\n",
        "# 5. 최적 subset 선택\n",
        "best_subset_idx = np.argmin(kl_divergences)\n",
        "best_subset = all_sampled_subsets[best_subset_idx]\n",
        "\n",
        "X_minority = X_train_scaled_df[y_train == 1]\n",
        "y_minority = y_train[y_train == 1]\n",
        "\n",
        "X_balanced = pd.concat([X_minority, best_subset])\n",
        "y_balanced = pd.concat([y_minority, pd.Series([0]*len(best_subset))])\n"
      ],
      "metadata": {
        "id": "VVCYhqqzCjhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "  model.fit(X_train,y_train)\n",
        "  pre = model.predict(X_test)\n",
        "\n",
        "  accuracy = accuracy_score(pre,y_test)\n",
        "  precision = precision_score(pre,y_test)\n",
        "  recall = recall_score(pre,y_test)\n",
        "  f1 = f1_score(pre,y_test)\n",
        "\n",
        "  sns.heatmap(confusion_matrix(pre,y_test),annot=True)\n",
        "  print(model)\n",
        "  print('Accuracy : ',accuracy,'Recall : ',recall, \"Precision : \",precision,\"F1 : \",f1)\n"
      ],
      "metadata": {
        "id": "obM59--aCpBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = lgb.LGBMClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "evaluate(clf)"
      ],
      "metadata": {
        "id": "gkIE1m1iR4Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = lgb.LGBMClassifier(random_state=42)\n",
        "clf.fit(X_balanced, y_balanced)\n",
        "pre = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(pre,y_test)\n",
        "precision = precision_score(pre,y_test)\n",
        "recall = recall_score(pre,y_test)\n",
        "f1 = f1_score(pre,y_test)\n",
        "\n",
        "sns.heatmap(confusion_matrix(pre,y_test),annot=True)\n",
        "print(clf)\n",
        "print('Accuracy : ',accuracy,'Recall : ',recall, \"Precision : \",precision,\"F1 : \",f1)"
      ],
      "metadata": {
        "id": "q0zHHDfuCuWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ARj-RT-QFrHy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}